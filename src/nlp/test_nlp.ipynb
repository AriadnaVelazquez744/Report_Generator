{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91e26d9",
   "metadata": {},
   "source": [
    "# Pruebas de los módulos NLP\n",
    "\n",
    "Ejecuta cada componente de `src/nlp` contra los artículos ubicados en `data_example/`, incorporando las etiquetas y clasificaciones generadas directamente sobre los archivos `.json` originales para que el vectorizador los tenga disponibles antes de la integración con el resto del sistema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045dbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artículos detectados: 9\n",
      "Idioma detectado para el primero: es\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from preprocessing import detect_language\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/lia/Escritorio/Proyectos/NLP/Report_Generator/Data/Data_articles\")\n",
    "DATA_DIR = PROJECT_ROOT / \"Data_articles4\"\n",
    "\n",
    "article_paths = sorted(DATA_DIR.glob(\"article_*.json\"))\n",
    "\n",
    "\n",
    "def load_article(path: Path) -> dict:\n",
    "    with open(path, encoding=\"utf-8\") as fh:\n",
    "        return json.load(fh)\n",
    "\n",
    "\n",
    "def persist_article(path: Path, data: dict) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as fh:\n",
    "        json.dump(data, fh, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def update_article(path: Path, updates: dict) -> dict:\n",
    "    data = load_article(path)\n",
    "    data.update(updates)\n",
    "    persist_article(path, data)\n",
    "    return data\n",
    "\n",
    "sample = load_article(article_paths[0])\n",
    "print(f\"Artículos detectados: {len(article_paths)}\")\n",
    "print(\"Idioma detectado para el primero:\", detect_language(sample.get(\"text\", \"\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b31a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence_count': 9, 'token_count': 237}\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import TextPreprocessor\n",
    "\n",
    "preprocessor = TextPreprocessor(use_spacy=False, remove_stopwords=True)\n",
    "preprocessing_results = {}\n",
    "\n",
    "for path in article_paths:\n",
    "    article = load_article(path)\n",
    "    text = article.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    processed = preprocessor.preprocess_full(text)\n",
    "    updates = {\n",
    "        \"preprocessing\": {\n",
    "            \"cleaned\": processed[\"cleaned\"],\n",
    "            \"tokens\": processed[\"tokens\"],\n",
    "            \"token_count\": processed[\"token_count\"],\n",
    "            \"sentence_count\": processed[\"sentence_count\"]\n",
    "        }\n",
    "    }\n",
    "    update_article(path, updates)\n",
    "    preprocessing_results[path.name] = {\n",
    "        \"token_count\": updates[\"preprocessing\"][\"token_count\"],\n",
    "        \"sentence_count\": updates[\"preprocessing\"][\"sentence_count\"]\n",
    "    }\n",
    "\n",
    "pprint(preprocessing_results.get(\"article_1.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db5c405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top_bigrams': {'FW-FW': 48,\n",
      "                 'FW-NN': 16,\n",
      "                 'JJ-NN': 16,\n",
      "                 'NN-NN': 77,\n",
      "                 'NNP-NN': 12},\n",
      " 'top_tags': {'FW': 78, 'JJ': 39, 'NN': 126, 'NNP': 32, 'NNS': 13},\n",
      " 'top_trigrams': {'FW-FW-FW': 33,\n",
      "                  'FW-NN-FW': 8,\n",
      "                  'JJ-NN-NN': 9,\n",
      "                  'NN-NN-NN': 54,\n",
      "                  'NNP-NN-NN': 10}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/lia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from pos_analyzer import POSAnalyzer\n",
    "\n",
    "pos_analyzer = POSAnalyzer()\n",
    "pos_results = {}\n",
    "\n",
    "for path in article_paths:\n",
    "    article = load_article(path)\n",
    "    text = article.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    pos_info = pos_analyzer.analyze(text)\n",
    "    patterns = pos_analyzer.get_top_patterns(pos_info, n=5)\n",
    "    updates = {\n",
    "        \"pos_analysis\": {\n",
    "            \"tag_freq\": pos_info[\"tag_freq\"],\n",
    "            \"bigram_freq\": pos_info[\"bigram_freq\"],\n",
    "            \"trigram_freq\": pos_info[\"trigram_freq\"],\n",
    "            \"top_patterns\": patterns\n",
    "        }\n",
    "    }\n",
    "    update_article(path, updates)\n",
    "    pos_results[path.name] = patterns\n",
    "\n",
    "pprint(pos_results.get(\"article_1.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c82a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunk_counts': {'ADJP': 7, 'NP': 71, 'PP': 6, 'VP': 14},\n",
      " 'top_rules': [{'frequency': 4,\n",
      "                'rule': \"PP -> ('de', 'IN') NP\",\n",
      "                'weight': 0.0374},\n",
      "               {'frequency': 2,\n",
      "                'rule': \"NP -> ('ambas', 'NN') ('naciones', 'NNS')\",\n",
      "                'weight': 0.0187},\n",
      "               {'frequency': 2,\n",
      "                'rule': \"ADJP -> ('Pezeshkian', 'JJ')\",\n",
      "                'weight': 0.0187},\n",
      "               {'frequency': 2,\n",
      "                'rule': \"PP -> ('en', 'IN') NP\",\n",
      "                'weight': 0.0187},\n",
      "               {'frequency': 1,\n",
      "                'rule': \"S -> NP VP ('de', 'FW') NP (',', ',') NP (',', ',') \"\n",
      "                        \"NP NP ('solidaridad', 'FW') NP ('las', 'FW') VP PP \"\n",
      "                        \"('en', 'FW') NP (',', ',') NP ('del', 'FW') NP ('.', \"\n",
      "                        \"'.')\",\n",
      "                'weight': 0.0093}]}\n"
     ]
    }
   ],
   "source": [
    "from grammar_analyzer import GrammarAnalyzer\n",
    "\n",
    "grammar_analyzer = GrammarAnalyzer()\n",
    "grammar_results = {}\n",
    "\n",
    "for path in article_paths:\n",
    "    article = load_article(path)\n",
    "    text = article.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    grammar_info = grammar_analyzer.analyze(text)\n",
    "    update_article(path, {\"grammar_analysis\": grammar_info})\n",
    "    grammar_results[path.name] = {\n",
    "        \"chunk_counts\": grammar_info[\"chunk_counts\"],\n",
    "        \"top_rules\": grammar_info[\"top_rules\"][:5]\n",
    "    }\n",
    "\n",
    "pprint(grammar_results.get(\"article_1.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d555238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONFLICTOS_ARMADOS_OPERACIONES',\n",
      " 'PAISES',\n",
      " 'INTEGRACION_LATINOAMERICANA',\n",
      " 'VIOLENCIA_CRIMEN_DERECHOS_HUMANOS',\n",
      " 'CIUDADES_CAPITALES',\n",
      " 'REGIONES_PROVINCIAS',\n",
      " 'RELACIONES_INTERNACIONALES',\n",
      " 'PROTESTAS_CONFLICTOS_SOCIALES',\n",
      " 'LEYES_NORMATIVAS',\n",
      " 'GOBIERNOS_OFICIALES',\n",
      " 'ELECCIONES_PROCESOS_POLITICOS',\n",
      " 'APROBACION_RECHAZO']\n",
      "['CULTURA_DEPORTE_SOCIEDAD',\n",
      " 'CONFLICTOS_ARMADOS_OPERACIONES',\n",
      " 'PAISES',\n",
      " 'INTEGRACION_LATINOAMERICANA',\n",
      " 'VIOLENCIA_CRIMEN_DERECHOS_HUMANOS',\n",
      " 'JURIDICO_LEGISLATIVO_CORRUPCION',\n",
      " 'ORGANISMOS_INTERNACIONALES',\n",
      " 'ACCIONES_LEGALES',\n",
      " 'RELACIONES_INTERNACIONALES',\n",
      " 'PROTESTAS_CONFLICTOS_SOCIALES',\n",
      " 'CIUDADES_CAPITALES',\n",
      " 'REGIONES_PROVINCIAS',\n",
      " 'ANTIIMPERIALISMO_SOBERANIA',\n",
      " 'ELECCIONES_PROCESOS_POLITICOS',\n",
      " 'APROBACION_RECHAZO']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from regex_annotator import RegexAnnotator\n",
    "\n",
    "regex_annotator = RegexAnnotator()\n",
    "regex_results = {}\n",
    "\n",
    "for path in article_paths:\n",
    "    article = load_article(path)\n",
    "    text = article.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    annotations = regex_annotator.annotate(text)\n",
    "    update_article(path, {\"regex_annotations\": annotations})\n",
    "    regex_results[path.name] = annotations[\"categories\"]\n",
    "\n",
    "pprint(regex_results.get(\"article_1.json\"))\n",
    "pprint(regex_results.get(\"article_11.json\"))\n",
    "pprint(regex_results.get(\"article_24.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4d59960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entidades': [{'nombre': 'Gobierno', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'Maduro', 'tipo': 'Persona'},\n",
      "               {'nombre': 'Nicolás Maduro', 'tipo': 'Persona'},\n",
      "               {'nombre': 'Teherán', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'Masoud Pezeshkian', 'tipo': 'Persona'},\n",
      "               {'nombre': 'Washington', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'Washington', 'tipo': 'Persona'},\n",
      "               {'nombre': 'Estados Unidos', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'América', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'Venezuela', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'Caracas', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'Estado', 'tipo': 'Organizacion'},\n",
      "               {'nombre': 'Gobierno iraní', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'Pezeshkian', 'tipo': 'Persona'},\n",
      "               {'nombre': 'Irán', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'el Caribe', 'tipo': 'Lugar'},\n",
      "               {'nombre': 'Donald Trump', 'tipo': 'Persona'}],\n",
      " 'relaciones': [{'objeto': 'Washington',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Venezuela'},\n",
      "                {'objeto': 'América',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Venezuela'},\n",
      "                {'objeto': 'el Caribe',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Venezuela'},\n",
      "                {'objeto': 'Washington',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Irán'},\n",
      "                {'objeto': 'América', 'relacion': 'SOSTENER', 'sujeto': 'Irán'},\n",
      "                {'objeto': 'el Caribe',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Irán'},\n",
      "                {'objeto': 'Washington',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Nicolás Maduro'},\n",
      "                {'objeto': 'América',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Nicolás Maduro'},\n",
      "                {'objeto': 'el Caribe',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Nicolás Maduro'},\n",
      "                {'objeto': 'Washington',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Masoud Pezeshkian'},\n",
      "                {'objeto': 'América',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Masoud Pezeshkian'},\n",
      "                {'objeto': 'el Caribe',\n",
      "                 'relacion': 'SOSTENER',\n",
      "                 'sujeto': 'Masoud Pezeshkian'},\n",
      "                {'objeto': 'Venezuela',\n",
      "                 'relacion': 'RECONOCER',\n",
      "                 'sujeto': 'Pezeshkian'},\n",
      "                {'objeto': 'Estado',\n",
      "                 'relacion': 'CONTINUAR_REAFIRMAR',\n",
      "                 'sujeto': 'Caracas'},\n",
      "                {'objeto': 'Estados Unidos',\n",
      "                 'relacion': 'DENUNCIAR_INSCRIBIR',\n",
      "                 'sujeto': 'Irán'},\n",
      "                {'objeto': 'Washington',\n",
      "                 'relacion': 'SE_ALAR',\n",
      "                 'sujeto': 'Teherán'},\n",
      "                {'objeto': 'Donald Trump',\n",
      "                 'relacion': 'CONDENAR',\n",
      "                 'sujeto': 'Gobierno iraní'}]}\n"
     ]
    }
   ],
   "source": [
    "from relation_extractor import RelationExtractor\n",
    "\n",
    "try:\n",
    "    relation_extractor = RelationExtractor()\n",
    "except ValueError as exc:\n",
    "    relation_extractor = None\n",
    "    print(f\"No se pueden extraer relaciones: {exc}\")\n",
    "\n",
    "relation_results = {}\n",
    "\n",
    "for path in article_paths:\n",
    "    article = load_article(path)\n",
    "    text = article.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    if relation_extractor:\n",
    "        rel_info = relation_extractor.extract(text)\n",
    "    else:\n",
    "        rel_info = {\"entidades\": [], \"relaciones\": [], \"error\": \"Modelo spaCy no disponible\"}\n",
    "    update_article(path, {\"knowledge_graph\": rel_info})\n",
    "    relation_results[path.name] = rel_info\n",
    "\n",
    "pprint(relation_results.get(\"article_1.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee51ebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compound': 0.34, 'label': 'positive', 'neg': 0.0, 'neu': 0.993, 'pos': 0.007}\n"
     ]
    }
   ],
   "source": [
    "from sentiment_analyzer import SentimentAnalyzer\n",
    "\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "sentiment_results = {}\n",
    "\n",
    "for path in article_paths:\n",
    "    article = load_article(path)\n",
    "    text = article.get(\"text\", \"\")\n",
    "    if not text:\n",
    "        continue\n",
    "    sentiment = sentiment_analyzer.analyze(text)\n",
    "    update_article(path, {\"sentiment\": sentiment})\n",
    "    sentiment_results[path.name] = sentiment\n",
    "\n",
    "pprint(sentiment_results.get(\"article_1.json\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
