from telegram.extract_data_tg import ScraperT
from src.summarization.summarizer import PersonalizedSummarizer, TextRankSummarizer
from src.recommendation.vectorizer import NewsVectorizer, UserProfileVectorizer
from src.recommendation.matcher import NewsMatcher
from src.recommendation.user_profile import UserProfileManager
from src.recommendation.report_generator import ReportGenerator
from src.nlp.preprocessing import TextPreprocessor
from src.nlp.regex_annotator import RegexAnnotator
import os 
import json
import numpy as np
import spacy

nlp  = spacy.load('es_core_news_lg')

path = 'Data_articles'
data_dirs = [x for x in os.listdir(path) if not x.startswith(".")]

def load_raw_data(limit=None):
    """Carga datos crudos de art√≠culos"""
    all_data = []
    count = 0
    for data_dir in data_dirs:
        dir_path = os.path.join(path, data_dir)
        for filename in os.listdir(dir_path):
            if filename.endswith('.json'):
                try:
                    with open(os.path.join(dir_path, filename)) as f:
                        article = json.load(f)
                        all_data.append(article)
                        count += 1
                        if limit and count >= limit:
                            return all_data
                except:
                    continue
    return all_data


def process_single_article(args):
    """Procesa un solo art√≠culo (para paralelizaci√≥n con threading)"""
    idx, article_data, nlp, text_processor, annotator = args
    
    try:
        text = article_data.get('text', '')
        if not text:
            return None
        
        # Procesar con spaCy
        doc = nlp(text)
        
        # Extraer entidades
        current_ents = [{'text': e.text, 'label': e.label_} for e in doc.ents]
        
        # Anotar con regex
        annotations = annotator.annotate(text)
        
        # Preprocesar texto
        clean_tokens = text_processor.preprocess(text)
        clean_text = ' '.join(clean_tokens)
        
        return {
            'id': idx,
            'title': article_data.get('title', 'Sin t√≠tulo'),
            'text': text,
            'clean_text': clean_text,
            'categories': annotations['categories'],
            'entidades': current_ents,
            'section': article_data.get('section', 'General'),
            'tags': article_data.get('tags', []),
            'url': article_data.get('url', ''),
            'source_metadata': article_data.get('source_metadata', {}),
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Error procesando art√≠culo {idx}: {e}")
        return None


def prepare_articles(raw_data, text_processor, annotator, news_vectorizer, nlp):
    """
    Prepara art√≠culos: extrae texto, categoriza con regex, limpia y vectoriza
    Usa ThreadPoolExecutor para paralelizaci√≥n real
    
    Returns:
        Lista de art√≠culos procesados con vectores y metadatos
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import tqdm
    import multiprocessing
    
    print(f"\nProcesando {len(raw_data)} art√≠culos...")
    
    # Preparar argumentos para cada art√≠culo
    tasks = [(i, article_data, nlp, text_processor, annotator) 
             for i, article_data in enumerate(raw_data)]
    
    articles = []
    clean_texts = []
    
    
    num_workers = multiprocessing.cpu_count()
    print(f"Procesando en paralelo con {num_workers} threads...")
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        # Enviar todas las tareas
        futures = {executor.submit(process_single_article, task): task[0] 
                   for task in tasks}
        
        # Recopilar resultados con barra de progreso
        for future in tqdm.tqdm(as_completed(futures), total=len(futures)):
            result = future.result()
            if result:
                articles.append(result)
                clean_texts.append(result['clean_text'])
    
    # Ordenar por ID original
    articles.sort(key=lambda x: x['id'])
    
    print(f"‚úÖ {len(articles)} art√≠culos procesados exitosamente")
    
    # Vectorizar todos los textos limpios
    print(f"\nVectorizando art√≠culos con TF-IDF...")
    article_matrix = news_vectorizer.fit_transform0(clean_texts)
    print(f"‚úÖ Matriz de art√≠culos: {article_matrix.shape}")
    
    # Agregar vectores a los art√≠culos
    for i, article in enumerate(articles):
        article['vector'] = article_matrix[i].tolist()
    
    return articles


def save_processed_articles(articles, filepath='processed_articles.json', vectorizer=None):
    """Guarda los art√≠culos procesados y el vectorizador en un √∫nico archivo JSON"""
    print(f"\nüíæ Guardando art√≠culos procesados en {filepath}...")
    
    data = {
         'vectorizer': vectorizer.to_dict() if vectorizer else {},
        'articles': articles
       
    }

    def make_json_serializable(obj):
        """Recursively convert numpy types and other non-JSON types to native Python types."""
        # Numpy scalar
        if isinstance(obj, (np.integer,)):
            return int(obj)
        if isinstance(obj, (np.floating,)):
            return float(obj)
        if isinstance(obj, (np.ndarray,)):
            return obj.tolist()

        # Basic types
        if isinstance(obj, (str, int, float, bool)) or obj is None:
            return obj

        # Datetime
        try:
            from datetime import datetime
            if isinstance(obj, datetime):
                return obj.isoformat()
        except Exception:
            pass

        # Dict
        if isinstance(obj, dict):
            return {str(k): make_json_serializable(v) for k, v in obj.items()}

        # Iterable (list/tuple)
        if isinstance(obj, (list, tuple)):
            return [make_json_serializable(v) for v in obj]

        # Fallback: try to cast to string
        try:
            return str(obj)
        except Exception:
            return None

    serializable_data = make_json_serializable(data)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(serializable_data, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Art√≠culos guardados exitosamente")


def load_processed_articles(filepath='processed_articles.json'):
    """Carga los art√≠culos procesados y vectorizador desde un archivo JSON"""
    if os.path.exists(filepath):
        print(f"\nüìÇ Cargando art√≠culos procesados desde {filepath}...")
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Compatibilidad con formato antiguo (solo lista de art√≠culos)
        if isinstance(data, list):
            print(f"‚úÖ {len(data)} art√≠culos cargados desde cache (formato antiguo)")
            return {'articles': data, 'vectorizer_data': None}
        
        articles = data.get('articles', [])
        vectorizer_data = data.get('vectorizer', {})
        print(f"‚úÖ {len(articles)} art√≠culos cargados desde cache")
        
        return {'articles': articles, 'vectorizer_data': vectorizer_data}
    return None


def create_simulated_users():
    """Crea perfiles de usuarios simulados con diferentes intereses basados en categor√≠as regex"""
    users = [
        {
            'name': 'Sof√≠a - Cr√≠tica de Arte',
            'profile_text': (
                'Soy una apasionada del arte contempor√°neo, las exposiciones y las galer√≠as. '
                'Me interesan las obras de artistas emergentes, el muralismo, la escultura y '
                'la fotograf√≠a art√≠stica. Sigo festivales culturales, bienales de arte, '
                'inauguraciones de museos y eventos de patrimonio cultural. Me fascina el '
                'teatro, la danza, el cine de autor y las manifestaciones art√≠sticas urbanas. '
                'Disfruto la m√∫sica cl√°sica, jazz, y expresiones folcl√≥ricas tradicionales.'
            )
        },
        {
            'name': 'Diego - Ambientalista',
            'profile_text': (
                'Me dedico a la conservaci√≥n ambiental y protecci√≥n de ecosistemas. '
                'Sigo temas de biodiversidad, especies en peligro de extinci√≥n, reservas naturales '
                'y parques nacionales. Me preocupan los desastres naturales como terremotos, '
                'inundaciones y huracanes. Denuncio la deforestaci√≥n, contaminaci√≥n de r√≠os, '
                'derrames de petr√≥leo y el cambio clim√°tico. Apoyo energ√≠as renovables, '
                'reciclaje y desarrollo sostenible. Me interesan proyectos de reforestaci√≥n '
                'y la protecci√≥n de oc√©anos y recursos h√≠dricos.'
            )
        },
        {
            'name': 'Laura - Educadora Cultural',
            'profile_text': (
                'Me apasiona la educaci√≥n, la literatura y la promoci√≥n cultural. '
                'Sigo lanzamientos de libros, ferias literarias, conciertos y recitales de poes√≠a. '
                'Me interesan programas educativos, becas, talleres art√≠sticos y actividades '
                'para ni√±os y j√≥venes. Apoyo bibliotecas comunitarias, centros culturales '
                'y espacios de creaci√≥n art√≠stica. Me gusta el teatro comunitario, '
                'la m√∫sica folcl√≥rica y las tradiciones ancestrales. Valoro la preservaci√≥n '
                'del patrimonio inmaterial y las lenguas ind√≠genas.'
            )
        },
        {
            'name': 'Mart√≠n - Fot√≥grafo de Naturaleza',
            'profile_text': (
                'Soy fot√≥grafo especializado en naturaleza, paisajes y vida silvestre. '
                'Me apasionan los parques naturales, santuarios de fauna, volcanes y monta√±as. '
                'Documento especies animales, aves migratorias, flora end√©mica y ecosistemas √∫nicos. '
                'Me interesan expediciones cient√≠ficas, descubrimientos de nuevas especies '
                'y proyectos de conservaci√≥n de h√°bitats. Sigo fen√≥menos naturales, auroras, '
                'eclipses y eventos astron√≥micos. Apoyo el turismo ecol√≥gico y responsable.'
            )
        },
        {
            'name': 'Carmen - Historiadora del Arte',
            'profile_text': (
                'Investigo historia del arte latinoamericano, arquitectura colonial y '
                'patrimonio hist√≥rico. Me fascinan las restauraciones de monumentos, '
                'excavaciones arqueol√≥gicas y descubrimientos de sitios hist√≥ricos. '
                'Estudio arte prehisp√°nico, culturas ind√≠genas y tradiciones artesanales. '
                'Me interesan museos, archivos hist√≥ricos, documentales culturales '
                'y la preservaci√≥n de arte sacro. Valoro el arte popular, textiles tradicionales '
                'y t√©cnicas ancestrales de pintura y cer√°mica.'
            )
        },
    ]
    return users


def main(nlp):

    print("=" * 80)
    print("SISTEMA DE RECOMENDACI√ìN DE NOTICIAS PERSONALIZADO")
    print("=" * 80)
    
    # Inicializar componentes
    text_processor = TextPreprocessor(use_spacy=False)
    annotator = RegexAnnotator()
    
    # Inicializar vectorizador de noticias (necesario siempre para perfiles de usuario)
    news_vectorizer = NewsVectorizer(max_features=3000, ngram_range=(1, 2))
    
    # Intentar cargar art√≠culos procesados desde cache
    processed_cache_file = 'processed_articles.json'
    cache_data = load_processed_articles(processed_cache_file)
    
    if cache_data is None:
        # No existe cache, procesar art√≠culos desde cero
        print("\nüìÇ Cargando art√≠culos crudos...")
        raw_data = load_raw_data()  # Cambia el limit o qu√≠talo para cargar todos
        print(f"‚úÖ {len(raw_data)} art√≠culos crudos cargados")
        
        # Preparar art√≠culos: categorizar, limpiar y vectorizar
        articles = prepare_articles(raw_data, text_processor, annotator, news_vectorizer, nlp)
        
        # Guardar en cache para futuras ejecuciones
        save_processed_articles(articles, processed_cache_file, vectorizer=news_vectorizer)
    else:
        # Cargar art√≠culos desde cache
        articles = cache_data['articles']
        vectorizer_data = cache_data['vectorizer_data']
        
        if vectorizer_data:
            # Cargar vectorizador desde datos en JSON
            print("\nüîß Restaurando vectorizador desde cache...")
            news_vectorizer = NewsVectorizer.from_dict(vectorizer_data)
            if news_vectorizer:
                print("‚úÖ Vectorizador restaurado")
            else:
                # Fallback si falla la deserializaci√≥n
                print("‚ö†Ô∏è  Error restaurando vectorizador, reajustando...")
                clean_texts = [article['clean_text'] for article in articles]
                news_vectorizer = NewsVectorizer(max_features=3000, ngram_range=(1, 2))
                news_vectorizer.fit0(clean_texts)
                print("‚úÖ Vectorizador ajustado")
        else:
            # Formato antiguo sin vectorizador, necesitamos ajustar
            print("\nüîß Ajustando vectorizador con art√≠culos del cache...")
            clean_texts = [article['clean_text'] for article in articles]
            news_vectorizer.fit0(clean_texts)
            print("‚úÖ Vectorizador ajustado")

    # Crear perfiles de usuarios simulados
    print("\nüë• Creando usuarios simulados...")
    simulated_users = create_simulated_users()
    
    # Inicializar componentes de recomendaci√≥n
    profile_vectorizer = UserProfileVectorizer(news_vectorizer)
    profile_manager = UserProfileManager(profile_vectorizer)
    matcher = NewsMatcher()
    
    # Inicializar resumidores
    base_summarizer = TextRankSummarizer(language="spanish")
    personalized_summarizer = PersonalizedSummarizer(base_summarizer)
    
    # Inicializar generador de reportes
    report_generator = ReportGenerator(personalized_summarizer)
    
    # Procesar cada usuario
    print("\n" + "=" * 80)
    print("GENERANDO RECOMENDACIONES PERSONALIZADAS")
    print("=" * 80)
    
    all_reports = []
    
    # Crear directorio para PDFs
    pdf_output_dir = "reportes_pdf"
    os.makedirs(pdf_output_dir, exist_ok=True)
    
    for user in simulated_users:
        print(f"\n{'='*80}")
        print(f"üë§ Usuario: {user['name']}")
        print(f"{'='*80}")
        print(f"üìù Perfil: {user['profile_text'][:100]}...")
        
        # Crear perfil del usuario con extracci√≥n de entidades
        user_profile = profile_manager.create_profile(user['profile_text'], nlp=nlp)
        
        print(f"\nüè∑Ô∏è  Categor√≠as de inter√©s detectadas: {user_profile['categories'][:8]}")
        print(f"üë§ Entidades de inter√©s: {[e['text'] for e in user_profile.get('entities', [])][:10]}")
        print(f"üìä Dimensi√≥n del vector de perfil: {len(user_profile['vector'])}")
        
        # Encontrar art√≠culos relevantes
        matches = matcher.match_articles(user_profile, articles, top_k=10)
        
        # Generar reporte personalizado
        report = report_generator.generate_report(matches, user_profile, max_articles=5)
        all_reports.append({
            'user_name': user['name'],
            'report': report
        })
        
        import time
        # Generar PDF
        # Crear nombre de archivo seguro
        safe_name = user['name'].replace(' ', '_').replace('-', '_').replace('/', '_')
        pdf_filename = f"{pdf_output_dir}/reporte_{safe_name}_{int(time.time())}.pdf"
        
        print(f"\nüìÑ Generando PDF...")
        if report_generator.generate_pdf(report, pdf_filename, user['name']):
            print(f"‚úÖ PDF guardado en: {pdf_filename}")
        else:
            print(f"‚ö†Ô∏è  No se pudo generar el PDF (instala reportlab: pip install reportlab)")
        
        print(f"\n{'='*80}\n")
    
    # Estad√≠sticas generales
    print("\n" + "=" * 80)
    print("üìä ESTAD√çSTICAS GENERALES")
    print("=" * 80)
    
    # Categor√≠as m√°s comunes en art√≠culos
    category_counts = {}
    for article in articles:
        for cat in article['categories']:
            category_counts[cat] = category_counts.get(cat, 0) + 1
    
    print("\nüèÜ Top 10 categor√≠as m√°s frecuentes en art√≠culos:")
    sorted_cats = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    for cat, count in sorted_cats:
        print(f"   {cat}: {count} art√≠culos")
    
    print(f"\nüìÅ Reportes PDF guardados en: {pdf_output_dir}/")
    print("\n‚úÖ Sistema completado exitosamente!")


if __name__ == "__main__":
    main(nlp)
